name: Benchmark

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
      - name: Set up Go
        uses: actions/setup-go@v6
        with:
          go-version-file: go.mod
        env:
          GOTOOLCHAIN: auto
          GOEXPERIMENT: jsonv2
      - name: Install Go dependencies and tools
        run: |
          go get .
          go install gotest.tools/gotestsum@latest
          go install github.com/mfridman/tparse@latest
      - name: Setup Neovim
        uses: rhysd/action-setup-vim@v1
        with:
          neovim: true
          version: stable

      - name: Install tree-sitter CLI
        run: |
          # Pin to v0.26.3 for reproducible builds (latest stable as of 2025-01)
          # Unpinned installs can cause CI failures when breaking changes are released
          npm install -g tree-sitter-cli@0.26.3

      - name: Cache Neovim plugins and parsers
        uses: actions/cache@v4
        with:
          path: |
            ~/.local/share/nvim
            ~/.config/nvim
          key: ${{ runner.os }}-neovim-tree-sitter-0.26.3-${{ hashFiles('.github/workflows/benchmark.yaml') }}
          restore-keys: |
            ${{ runner.os }}-neovim-

      - name: Install nvim-treesitter and parsers
        run: |
          set -e  # Exit on any error

          # Verify tree-sitter CLI is available
          echo "Verifying tree-sitter CLI..."
          tree-sitter --version

          # Install nvim-treesitter plugin
          NVIM_SITE="$HOME/.local/share/nvim/site/pack/plugins/start"
          PARSER_INSTALL_DIR="$HOME/.local/share/nvim/site"

          echo "Installing nvim-treesitter plugin..."
          mkdir -p "$NVIM_SITE"
          if [ ! -d "$NVIM_SITE/nvim-treesitter" ]; then
            git clone --depth=1 https://github.com/nvim-treesitter/nvim-treesitter.git \
              "$NVIM_SITE/nvim-treesitter"
          fi

          # Create install script using modern nvim-treesitter API
          cat > /tmp/install_parsers.lua << 'LUA'
          -- Set packpath to load nvim-treesitter
          vim.opt.packpath = { vim.fn.expand("~/.local/share/nvim/site") }
          vim.cmd.packloadall()

          -- Set up nvim-treesitter with install_dir
          local ts = require('nvim-treesitter')
          ts.setup {
            install_dir = vim.fn.expand("~/.local/share/nvim/site")
          }

          -- Install parsers programmatically
          print("Installing HTML and TypeScript parsers...")
          local ok, err = pcall(function()
            ts.install({ 'html', 'typescript' }):wait(120000)
          end)

          if not ok then
            print("ERROR: Parser installation failed: " .. tostring(err))
            os.exit(1)
          end

          print("✅ Parsers installed successfully")
          os.exit(0)
          LUA

          # Run parser installation
          echo "Installing HTML and TypeScript parsers..."
          nvim --headless -u /tmp/install_parsers.lua

          # Verify parsers were installed
          echo "Verifying parsers..."
          if [ -f "$PARSER_INSTALL_DIR/parser/html.so" ] && [ -f "$PARSER_INSTALL_DIR/parser/typescript.so" ]; then
            echo "✅ Tree-sitter parsers installed successfully:"
            ls -lh "$PARSER_INSTALL_DIR/parser/html.so" "$PARSER_INSTALL_DIR/parser/typescript.so"
          else
            echo "ERROR: Parsers not found in $PARSER_INSTALL_DIR/parser/"
            echo "Searching for all .so files:"
            find "$HOME/.local/share/nvim" -name "*.so" -type f 2>/dev/null || echo "No .so files found"
            exit 1
          fi

      - name: Run benchmarks on PR branch
        id: bench-pr
        run: scripts/benchmark-ci-run.sh pr

      - name: Build CEM binary for LSP benchmarks
        run: make build

      - name: Run LSP benchmarks on PR branch
        id: bench-lsp-pr
        timeout-minutes: 5
        run: scripts/lsp-benchmark-ci-run.sh pr

      - name: Checkout target branch (base)
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          clean: false

      - name: Prewarm benchmarks on base branch
        run: |
          if make -n bench-generate >/dev/null 2>&1; then
            make bench-generate || true
          else
            echo "::warning::Base branch does not have 'make bench-generate' target"
          fi

      - name: Run benchmarks on base branch
        id: bench-base
        run: |
          if [ -f "scripts/benchmark-ci-run.sh" ]; then
            scripts/benchmark-ci-run.sh base
          else
            echo "::warning::Base branch does not have benchmark script (first run)"
            echo '{}' > base_bench.json
            echo "0" > base_bench_time.txt
            echo "First run - no base results" > base_bench.txt
          fi

      - name: Build CEM binary for LSP benchmarks (base)
        run: |
          if [ -f "Makefile" ] && make -n build >/dev/null 2>&1; then
            make build
          else
            echo "::warning::Base branch does not have 'make build' target - skipping LSP benchmarks"
            echo '{"benchmarks":{}, "error": "build_unavailable"}' > base_lsp_bench.json
          fi

      - name: Run LSP benchmarks on base branch
        id: bench-lsp-base
        timeout-minutes: 5
        run: |
          if [ -f "scripts/lsp-benchmark-ci-run.sh" ] && [ -f "dist/cem" ]; then
            scripts/lsp-benchmark-ci-run.sh base
          else
            echo "::warning::Base branch does not have LSP benchmark infrastructure (first run)"
            echo '{"benchmarks":{}, "error": "script_not_found", "note": "First benchmark run - base branch does not have benchmark infrastructure"}' > base_lsp_bench.json
          fi

      - name: Restore PR context for comparison
        uses: actions/checkout@v4
        with:
          ref: ${{ github.sha }}
          clean: false

      - name: Compare benchmark results
        run: scripts/compare-benchmarks.sh

      - name: Compare LSP benchmark results
        run: scripts/compare-lsp-benchmarks.sh

      - name: Combine benchmark reports
        run: scripts/combine-benchmark-reports.sh

      - name: Find previous benchmark comment
        id: find-comment
        uses: peter-evans/find-comment@v3
        with:
          issue-number: ${{ github.event.pull_request.number }}
          comment-author: "github-actions[bot]"
          body-includes: "### Generate Benchmarks"

      - name: Create or update sticky benchmark comment
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-file: combined_bench_report.md
          comment-id: ${{ steps.find-comment.outputs.comment-id }}
          edit-mode: replace

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-artifacts
          path: |
            pr_bench.txt
            base_bench.txt
            pr_bench_time.txt
            base_bench_time.txt
            pr_bench.json
            base_bench.json
            pr_lsp_bench.json
            base_lsp_bench.json
            bench_report.md
            lsp_bench_report.md
            combined_bench_report.md
