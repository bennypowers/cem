name: Benchmark

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
      - name: Set up Go
        uses: actions/setup-go@v6
        with:
          go-version-file: go.mod
        env:
          GOTOOLCHAIN: auto
          GOEXPERIMENT: jsonv2
      - name: Install Go dependencies and tools
        run: |
          go get .
          go install gotest.tools/gotestsum@latest
          go install github.com/mfridman/tparse@latest
      - name: Setup Neovim
        uses: rhysd/action-setup-vim@v1
        with:
          neovim: true
          version: stable
      - name: Run benchmarks on PR branch
        id: bench-pr
        run: scripts/benchmark-ci-run.sh pr

      - name: Build CEM binary for LSP benchmarks
        run: make build

      - name: Run LSP benchmarks on PR branch
        id: bench-lsp-pr
        timeout-minutes: 5
        run: scripts/lsp-benchmark-ci-run.sh pr

      - name: Checkout target branch (base)
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          clean: false

      - name: Prewarm benchmarks on base branch
        run: make bench

      - name: Run benchmarks on base branch
        id: bench-base
        run: scripts/benchmark-ci-run.sh base

      - name: Build CEM binary for LSP benchmarks (base)
        run: make build

      - name: Run LSP benchmarks on base branch
        id: bench-lsp-base
        timeout-minutes: 5
        run: scripts/lsp-benchmark-ci-run.sh base

      - name: Restore PR context for comparison
        uses: actions/checkout@v4
        with:
          ref: ${{ github.sha }}
          clean: false

      - name: Compare benchmark results
        run: scripts/compare-benchmarks.sh

      - name: Compare LSP benchmark results
        run: scripts/compare-lsp-benchmarks.sh

      - name: Combine benchmark reports
        run: scripts/combine-benchmark-reports.sh

      - name: Find previous benchmark comment
        id: find-comment
        uses: peter-evans/find-comment@v3
        with:
          issue-number: ${{ github.event.pull_request.number }}
          comment-author: "github-actions[bot]"
          body-includes: "### Benchmark Summary"

      - name: Create or update sticky benchmark comment
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-file: combined_bench_report.md
          comment-id: ${{ steps.find-comment.outputs.comment-id }}
          edit-mode: replace

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-artifacts
          path: |
            pr_bench.txt
            base_bench.txt
            pr_bench_time.txt
            base_bench_time.txt
            pr_bench.json
            base_bench.json
            pr_lsp_bench.json
            base_lsp_bench.json
            bench_report.md
            lsp_bench_report.md
            combined_bench_report.md
